{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-25T03:17:46.555902Z",
     "iopub.status.busy": "2025-05-25T03:17:46.555713Z",
     "iopub.status.idle": "2025-05-25T03:17:46.821053Z",
     "shell.execute_reply": "2025-05-25T03:17:46.819760Z",
     "shell.execute_reply.started": "2025-05-25T03:17:46.555881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:17:59.175382Z",
     "iopub.status.busy": "2025-05-25T03:17:59.175153Z",
     "iopub.status.idle": "2025-05-25T03:19:24.146741Z",
     "shell.execute_reply": "2025-05-25T03:19:24.145959Z",
     "shell.execute_reply.started": "2025-05-25T03:17:59.175361Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  DEPRECATION: Building 'rouge_score' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'rouge_score'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q torch transformers datasets accelerate peft evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:19:48.231927Z",
     "iopub.status.busy": "2025-05-25T03:19:48.231661Z",
     "iopub.status.idle": "2025-05-25T03:19:48.239135Z",
     "shell.execute_reply": "2025-05-25T03:19:48.238471Z",
     "shell.execute_reply.started": "2025-05-25T03:19:48.231904Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in dataset folder:\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/meddialogue'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/meddialogue\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles in dataset folder:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/meddialogue'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Replace with your actual dataset folder name\n",
    "dataset_path = '/kaggle/input/meddialogue'\n",
    "\n",
    "print(\"Files in dataset folder:\")\n",
    "print(os.listdir(dataset_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:19:53.744075Z",
     "iopub.status.busy": "2025-05-25T03:19:53.743861Z",
     "iopub.status.idle": "2025-05-25T03:19:53.795943Z",
     "shell.execute_reply": "2025-05-25T03:19:53.795312Z",
     "shell.execute_reply.started": "2025-05-25T03:19:53.744061Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/meddialogue/english-train.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/meddialogue/english-train.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/meddialogue/english-dev.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/code/Mediguide/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/meddialogue/english-train.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/kaggle/input/meddialogue/english-train.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(\"/kaggle/input/meddialogue/english-dev.json\", \"r\") as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "with open(\"/kaggle/input/meddialogue/english-test.json\", \"r\") as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:19:56.005663Z",
     "iopub.status.busy": "2025-05-25T03:19:56.005106Z",
     "iopub.status.idle": "2025-05-25T03:19:57.246908Z",
     "shell.execute_reply": "2025-05-25T03:19:57.246317Z",
     "shell.execute_reply.started": "2025-05-25T03:19:56.005648Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m             formatted\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: text})\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m formatted\n\u001b[0;32m---> 14\u001b[0m train_samples \u001b[38;5;241m=\u001b[39m format_dialogue_data(\u001b[43mtrain_data\u001b[49m)\n\u001b[1;32m     15\u001b[0m val_samples \u001b[38;5;241m=\u001b[39m format_dialogue_data(val_data)\n\u001b[1;32m     16\u001b[0m test_samples \u001b[38;5;241m=\u001b[39m format_dialogue_data(test_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def format_dialogue_data(data):\n",
    "    formatted = []\n",
    "    for item in data:\n",
    "        utterances = item.get(\"utterances\", [])\n",
    "        if len(utterances) >= 2 and utterances[0].lower().startswith(\"patient\") and utterances[1].lower().startswith(\"doctor\"):\n",
    "            question = utterances[0].replace(\"patient:\", \"\").strip()\n",
    "            answer = utterances[1].replace(\"doctor:\", \"\").strip()\n",
    "            text = f\"Patient: {question}\\nDoctor: {answer} [Disclaimer: This is not medical advice.]\"\n",
    "            formatted.append({\"text\": text})\n",
    "    return formatted\n",
    "\n",
    "train_samples = format_dialogue_data(train_data)\n",
    "val_samples = format_dialogue_data(val_data)\n",
    "test_samples = format_dialogue_data(test_data)\n",
    "\n",
    "train_dataset = Dataset.from_list(train_samples)\n",
    "val_dataset = Dataset.from_list(val_samples)\n",
    "test_dataset = Dataset.from_list(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:20:01.369588Z",
     "iopub.status.busy": "2025-05-25T03:20:01.369340Z",
     "iopub.status.idle": "2025-05-25T03:20:32.396966Z",
     "shell.execute_reply": "2025-05-25T03:20:32.396265Z",
     "shell.execute_reply.started": "2025-05-25T03:20:01.369570Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb6b21d667246cd82180254b8edf229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 03:20:12.683969: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748143212.853020      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748143212.902070      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccda98582f974a049c9bbe284e4ef6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f210c56d5e242b2a467442d2a4d035e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421ebf95d72944b1b7f0110243ffbc77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c2790890a84716a03edeac00eb7a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9dabca4e7c49caa15177db313e956b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b0d93752f44713bbc5e6565f55df74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# To resume from checkpoint, change this path\n",
    "model_path = \"gpt2-medium\"  # or \"mediguide_checkpoint_epoch2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:20:47.742771Z",
     "iopub.status.busy": "2025-05-25T03:20:47.742028Z",
     "iopub.status.idle": "2025-05-25T03:20:48.220291Z",
     "shell.execute_reply": "2025-05-25T03:20:48.219390Z",
     "shell.execute_reply.started": "2025-05-25T03:20:47.742755Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1336cd1b6b84e55bf6469454db1cb62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/482 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8b33b7d1c64c0291a88582ae813037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78028d899a3d4e4292abe58cb628d9dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/61 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:20:50.530246Z",
     "iopub.status.busy": "2025-05-25T03:20:50.529629Z",
     "iopub.status.idle": "2025-05-25T03:20:52.211266Z",
     "shell.execute_reply": "2025-05-25T03:20:52.210548Z",
     "shell.execute_reply.started": "2025-05-25T03:20:50.530222Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "train_loader = DataLoader(tokenized_train, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:20:56.021007Z",
     "iopub.status.busy": "2025-05-25T03:20:56.020347Z",
     "iopub.status.idle": "2025-05-25T03:20:56.048253Z",
     "shell.execute_reply": "2025-05-25T03:20:56.047745Z",
     "shell.execute_reply.started": "2025-05-25T03:20:56.020987Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 2\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:20:57.928921Z",
     "iopub.status.busy": "2025-05-25T03:20:57.928689Z",
     "iopub.status.idle": "2025-05-25T05:40:03.390696Z",
     "shell.execute_reply": "2025-05-25T05:40:03.387646Z",
     "shell.execute_reply.started": "2025-05-25T03:20:57.928906Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/241 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Epoch 1: 100%|██████████| 241/241 [1:09:43<00:00, 17.36s/it, loss=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 241/241 [1:09:15<00:00, 17.24s/it, loss=0.739]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    loop = tqdm(train_loader)\n",
    "    for batch in loop:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"input_ids\"]\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # ✅ Save after each epoch\n",
    "    checkpoint_path = f\"mediguide_checkpoint_epoch{epoch+1}\"\n",
    "    model.save_pretrained(checkpoint_path)\n",
    "    tokenizer.save_pretrained(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T05:45:08.508677Z",
     "iopub.status.busy": "2025-05-25T05:45:08.508336Z",
     "iopub.status.idle": "2025-05-25T05:46:30.079563Z",
     "shell.execute_reply": "2025-05-25T05:46:30.078672Z",
     "shell.execute_reply.started": "2025-05-25T05:45:08.508615Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/mediguide_checkpoint_epoch2/ (stored 0%)\n",
      "  adding: kaggle/working/mediguide_checkpoint_epoch2/vocab.json (deflated 59%)\n",
      "  adding: kaggle/working/mediguide_checkpoint_epoch2/tokenizer.json (deflated 82%)\n",
      "  adding: kaggle/working/mediguide_checkpoint_epoch2/model.safetensors (deflated 7%)\n",
      "  adding: kaggle/working/mediguide_checkpoint_epoch2/tokenizer_config.json (deflated 54%)\n",
      "  adding: kaggle/working/mediguide_checkpoint_epoch2/merges.txt (deflated 53%)\n",
      "  adding: kaggle/working/mediguide_checkpoint_epoch2/config.json (deflated 52%)\n",
      "  adding: kaggle/working/mediguide_checkpoint_epoch2/special_tokens_map.json (deflated 60%)\n",
      "  adding: kaggle/working/mediguide_checkpoint_epoch2/generation_config.json (deflated 24%)\n"
     ]
    }
   ],
   "source": [
    "# ✅ Save model and tokenizer after Epoch 2\n",
    "model.save_pretrained(\"/kaggle/working/mediguide_checkpoint_epoch2\")\n",
    "tokenizer.save_pretrained(\"/kaggle/working/mediguide_checkpoint_epoch2\")\n",
    "\n",
    "# ✅ Zip the folder so it can be downloaded\n",
    "!zip -r /kaggle/working/mediguide_checkpoint_epoch2.zip /kaggle/working/mediguide_checkpoint_epoch2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T05:58:47.413928Z",
     "iopub.status.busy": "2025-05-25T05:58:47.413402Z",
     "iopub.status.idle": "2025-05-25T05:58:47.418005Z",
     "shell.execute_reply": "2025-05-25T05:58:47.417503Z",
     "shell.execute_reply.started": "2025-05-25T05:58:47.413910Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='mediguide_checkpoint_epoch2.zip' target='_blank'>mediguide_checkpoint_epoch2.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/mediguide_checkpoint_epoch2.zip"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'mediguide_checkpoint_epoch2.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T06:33:32.873486Z",
     "iopub.status.busy": "2025-05-25T06:33:32.873182Z",
     "iopub.status.idle": "2025-05-25T06:33:33.175407Z",
     "shell.execute_reply": "2025-05-25T06:33:33.174680Z",
     "shell.execute_reply.started": "2025-05-25T06:33:32.873468Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the saved model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mediguide_checkpoint_epoch2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mediguide_checkpoint_epoch2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T06:33:43.748393Z",
     "iopub.status.busy": "2025-05-25T06:33:43.747977Z",
     "iopub.status.idle": "2025-05-25T06:33:43.784461Z",
     "shell.execute_reply": "2025-05-25T06:33:43.783809Z",
     "shell.execute_reply.started": "2025-05-25T06:33:43.748378Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load validation data\n",
    "with open(\"/kaggle/input/meddialogue/english-dev.json\", \"r\") as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "# Format it into the proper prompt/response format\n",
    "def format_dialogue_data(data):\n",
    "    formatted = []\n",
    "    for item in data:\n",
    "        utterances = item.get(\"utterances\", [])\n",
    "        if len(utterances) >= 2 and utterances[0].lower().startswith(\"patient\") and utterances[1].lower().startswith(\"doctor\"):\n",
    "            question = utterances[0].replace(\"patient:\", \"\").strip()\n",
    "            answer = utterances[1].replace(\"doctor:\", \"\").strip()\n",
    "            text = f\"Patient: {question}\\nDoctor:\"\n",
    "            formatted.append({\"prompt\": text, \"reference\": answer})\n",
    "    return formatted\n",
    "\n",
    "eval_data = format_dialogue_data(val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T06:33:58.478617Z",
     "iopub.status.busy": "2025-05-25T06:33:58.478321Z",
     "iopub.status.idle": "2025-05-25T06:33:58.482443Z",
     "shell.execute_reply": "2025-05-25T06:33:58.481819Z",
     "shell.execute_reply.started": "2025-05-25T06:33:58.478575Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eval_data = eval_data[:100]  # Use 100 samples for ROUGE evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T06:35:16.886636Z",
     "iopub.status.busy": "2025-05-25T06:35:16.886324Z",
     "iopub.status.idle": "2025-05-25T06:40:05.214054Z",
     "shell.execute_reply": "2025-05-25T06:40:05.213330Z",
     "shell.execute_reply.started": "2025-05-25T06:35:16.886588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "generated_texts = []\n",
    "reference_texts = []\n",
    "\n",
    "for sample in eval_data:\n",
    "    inputs = tokenizer(sample[\"prompt\"], return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "    # ✅ Use attention_mask + generate only new tokens (e.g., 50 tokens after prompt)\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=50,  # Only generate up to 50 new tokens after the prompt\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode and extract the doctor's response\n",
    "    generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    if \"Doctor:\" in generated:\n",
    "        generated_answer = generated.split(\"Doctor:\")[-1].strip()\n",
    "    else:\n",
    "        generated_answer = generated.strip()\n",
    "\n",
    "    generated_texts.append(generated_answer)\n",
    "    reference_texts.append(sample[\"reference\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T06:41:30.548043Z",
     "iopub.status.busy": "2025-05-25T06:41:30.547783Z",
     "iopub.status.idle": "2025-05-25T06:41:31.283898Z",
     "shell.execute_reply": "2025-05-25T06:41:31.283302Z",
     "shell.execute_reply.started": "2025-05-25T06:41:30.548027Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Evaluation Results:\n",
      "rouge1: 0.1990\n",
      "rouge2: 0.0536\n",
      "rougeL: 0.1481\n",
      "rougeLsum: 0.1480\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "results = rouge.compute(predictions=generated_texts, references=reference_texts)\n",
    "\n",
    "# Display results\n",
    "print(\"ROUGE Evaluation Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T06:45:41.039431Z",
     "iopub.status.busy": "2025-05-25T06:45:41.038902Z",
     "iopub.status.idle": "2025-05-25T06:48:24.299999Z",
     "shell.execute_reply": "2025-05-25T06:48:24.299338Z",
     "shell.execute_reply.started": "2025-05-25T06:45:41.039417Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.2206\n",
      "Perplexity (PPL): 3.39\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "num_batches = 0\n",
    "\n",
    "# You can reduce batch size or sample count if this is slow\n",
    "for sample in eval_data[:100]:  # Evaluate on first 100 samples\n",
    "    inputs = tokenizer(\n",
    "        sample[\"prompt\"] + sample[\"reference\"],\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "# Final perplexity\n",
    "avg_loss = total_loss / num_batches\n",
    "perplexity = math.exp(avg_loss)\n",
    "\n",
    "print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "print(f\"Perplexity (PPL): {perplexity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2623467,
     "sourceId": 4748193,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
