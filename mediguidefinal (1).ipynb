{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4748193,"sourceType":"datasetVersion","datasetId":2623467},{"sourceId":11965213,"sourceType":"datasetVersion","datasetId":7523876}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-05T13:22:17.862268Z","iopub.execute_input":"2025-06-05T13:22:17.862778Z","iopub.status.idle":"2025-06-05T13:22:17.909488Z","shell.execute_reply.started":"2025-06-05T13:22:17.862718Z","shell.execute_reply":"2025-06-05T13:22:17.908479Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/mediguide-checkpoint-chatbot/mediguide_checkpoint_epoch2/config.json\n/kaggle/input/mediguide-checkpoint-chatbot/mediguide_checkpoint_epoch2/merges.txt\n/kaggle/input/mediguide-checkpoint-chatbot/mediguide_checkpoint_epoch2/vocab.json\n/kaggle/input/mediguide-checkpoint-chatbot/mediguide_checkpoint_epoch2/tokenizer_config.json\n/kaggle/input/mediguide-checkpoint-chatbot/mediguide_checkpoint_epoch2/model.safetensors\n/kaggle/input/mediguide-checkpoint-chatbot/mediguide_checkpoint_epoch2/special_tokens_map.json\n/kaggle/input/mediguide-checkpoint-chatbot/mediguide_checkpoint_epoch2/generation_config.json\n/kaggle/input/meddialogue/test_data.json\n/kaggle/input/meddialogue/english-dev.json\n/kaggle/input/meddialogue/healthcaremagic_dialogue_3.txt\n/kaggle/input/meddialogue/train_data.json\n/kaggle/input/meddialogue/healthcaremagic_dialogue_4.txt\n/kaggle/input/meddialogue/healthcaremagic_dialogue_2.txt\n/kaggle/input/meddialogue/healthcaremagic_dialogue_1.txt\n/kaggle/input/meddialogue/english-train.json\n/kaggle/input/meddialogue/english-test.json\n/kaggle/input/meddialogue/icliniq_dialogue.txt\n/kaggle/input/meddialogue/validate_data.json\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install -q torch transformers datasets evaluate rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T10:47:20.340617Z","iopub.execute_input":"2025-06-06T10:47:20.340868Z","iopub.status.idle":"2025-06-06T10:48:52.599632Z","shell.execute_reply.started":"2025-06-06T10:47:20.340836Z","shell.execute_reply":"2025-06-06T10:48:52.598792Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport json\n\n# Importing MedDialog dataset \ndataset_path = '/kaggle/input/meddialogue'\n\n# Check dataset is uploaded or not\nprint(\"Files in dataset folder:\")\nprint(os.listdir(dataset_path))\n\n# Loading train, validation, and test sets\nwith open(f\"{dataset_path}/english-train.json\", \"r\") as f:\n    train_data = json.load(f)\nwith open(f\"{dataset_path}/english-dev.json\", \"r\") as f:\n    val_data = json.load(f)\nwith open(f\"{dataset_path}/english-test.json\", \"r\") as f:\n    test_data = json.load(f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T10:51:50.493498Z","iopub.execute_input":"2025-06-06T10:51:50.493768Z","iopub.status.idle":"2025-06-06T10:51:50.527068Z","shell.execute_reply.started":"2025-06-06T10:51:50.493737Z","shell.execute_reply":"2025-06-06T10:51:50.526213Z"}},"outputs":[{"name":"stdout","text":"Files in dataset folder:\n['test_data.json', 'english-dev.json', 'healthcaremagic_dialogue_3.txt', 'train_data.json', 'healthcaremagic_dialogue_4.txt', 'healthcaremagic_dialogue_2.txt', 'healthcaremagic_dialogue_1.txt', 'english-train.json', 'english-test.json', 'icliniq_dialogue.txt', 'validate_data.json']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datasets import Dataset\n\n#Extract question - answers pairs from the3 datasets\ndef format_dialogue_data(data):\n    formatted = []\n    for item in data:\n        utterances = item.get(\"utterances\", [])\n        if len(utterances) >= 2 and utterances[0].lower().startswith(\"patient\") and utterances[1].lower().startswith(\"doctor\"):\n            question = utterances[0].replace(\"patient:\", \"\").strip()\n            answer = utterances[1].replace(\"doctor:\", \"\").strip()\n            text = f\"Patient: {question}\\nDoctor: {answer} [Disclaimer: This is not medical advice.]\"\n            formatted.append({\"text\": text})\n    return formatted\n\ntrain_samples = format_dialogue_data(train_data)\nval_samples = format_dialogue_data(val_data)\ntest_samples = format_dialogue_data(test_data)\n\ntrain_dataset = Dataset.from_list(train_samples)\nval_dataset = Dataset.from_list(val_samples)\ntest_dataset = Dataset.from_list(test_samples)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T10:52:04.277854Z","iopub.execute_input":"2025-06-06T10:52:04.278561Z","iopub.status.idle":"2025-06-06T10:52:06.251102Z","shell.execute_reply.started":"2025-06-06T10:52:04.278527Z","shell.execute_reply":"2025-06-06T10:52:06.250462Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Import classes from HuggingFace Transformers and PyTorch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# using gpt-2 medium\nmodel_name = \"gpt2-medium\"\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n\ntokenizer.pad_token = tokenizer.eos_token\nmodel.resize_token_embeddings(len(tokenizer))\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T10:52:37.206704Z","iopub.execute_input":"2025-06-06T10:52:37.207493Z","iopub.status.idle":"2025-06-06T10:53:15.282021Z","shell.execute_reply.started":"2025-06-06T10:52:37.207464Z","shell.execute_reply":"2025-06-06T10:53:15.281190Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0df77011bca44ea9b8d3d57601fe4f38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"910b512741e84447946c3c5a83f7ce3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09b75a81654246dfaaf230f010a717ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a1ce7ac185548d189ed6a9cb96aa487"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4bae7091f7a4289a608daa8adae756e"}},"metadata":{}},{"name":"stderr","text":"2025-06-06 10:52:54.472860: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749207174.730142      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749207174.804248      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5eb73f253f94b149407637192b10276"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f59d7aaf4824adfa3b346b2d4fbafca"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 1024)\n    (wpe): Embedding(1024, 1024)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-23): 24 x GPT2Block(\n        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D(nf=3072, nx=1024)\n          (c_proj): Conv1D(nf=1024, nx=1024)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=4096, nx=1024)\n          (c_proj): Conv1D(nf=1024, nx=4096)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Tokenize function for all samples\ndef tokenize_function(example):\n    return tokenizer(\n        example[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512,\n        return_tensors=\"pt\"\n    )\n\n# Apply tokenization\ntokenized_train = train_dataset.map(tokenize_function, batched=True)\ntokenized_val = val_dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T10:54:15.709272Z","iopub.execute_input":"2025-06-06T10:54:15.710319Z","iopub.status.idle":"2025-06-06T10:54:16.117082Z","shell.execute_reply.started":"2025-06-06T10:54:15.710289Z","shell.execute_reply":"2025-06-06T10:54:16.116251Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/482 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f859e09c97a440559e680e65bac84ee5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/60 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dd7dec2f28a4847b8a1b09eca4ac9e1"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\ntrain_loader = DataLoader(tokenized_train, batch_size=2, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T10:54:23.974824Z","iopub.execute_input":"2025-06-06T10:54:23.975483Z","iopub.status.idle":"2025-06-06T10:54:23.981036Z","shell.execute_reply.started":"2025-06-06T10:54:23.975453Z","shell.execute_reply":"2025-06-06T10:54:23.980250Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#Now train the model \nfrom torch.optim import AdamW\nfrom transformers import get_scheduler\n\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nnum_epochs = 2\nnum_training_steps = len(train_loader) * num_epochs\n\nlr_scheduler = get_scheduler(\n    name=\"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T10:55:22.451352Z","iopub.execute_input":"2025-06-06T10:55:22.452129Z","iopub.status.idle":"2025-06-06T10:55:22.501412Z","shell.execute_reply.started":"2025-06-06T10:55:22.452088Z","shell.execute_reply":"2025-06-06T10:55:22.500520Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from tqdm import tqdm\n\nmodel.train()\n\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}\")\n    loop = tqdm(train_loader)\n    for batch in loop:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n            labels=batch[\"input_ids\"]\n        )\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n        loop.set_description(f\"Epoch {epoch+1}\")\n        loop.set_postfix(loss=loss.item())\n\n    # ✅ Save after each epoch\n    checkpoint_path = f\"mediguide_checkpoint_epoch{epoch+1}\"\n    model.save_pretrained(checkpoint_path)\n    tokenizer.save_pretrained(checkpoint_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:20:57.928689Z","iopub.execute_input":"2025-05-25T03:20:57.928921Z","iopub.status.idle":"2025-05-25T05:40:03.390696Z","shell.execute_reply.started":"2025-05-25T03:20:57.928906Z","shell.execute_reply":"2025-05-25T05:40:03.387646Z"}},"outputs":[{"name":"stdout","text":"Epoch 1\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/241 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\nEpoch 1: 100%|██████████| 241/241 [1:09:43<00:00, 17.36s/it, loss=1]    \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 241/241 [1:09:15<00:00, 17.24s/it, loss=0.739]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"#Save model to avoid run again the two epochs\nmodel.save_pretrained(\"/kaggle/working/mediguide_checkpoint_epoch2\")\ntokenizer.save_pretrained(\"/kaggle/working/mediguide_checkpoint_epoch2\")\n\n!zip -r /kaggle/working/mediguide_checkpoint_epoch2.zip /kaggle/working/mediguide_checkpoint_epoch2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:45:08.508336Z","iopub.execute_input":"2025-05-25T05:45:08.508677Z","iopub.status.idle":"2025-05-25T05:46:30.079563Z","shell.execute_reply.started":"2025-05-25T05:45:08.508615Z","shell.execute_reply":"2025-05-25T05:46:30.078672Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  adding: kaggle/working/mediguide_checkpoint_epoch2/ (stored 0%)\n  adding: kaggle/working/mediguide_checkpoint_epoch2/vocab.json (deflated 59%)\n  adding: kaggle/working/mediguide_checkpoint_epoch2/tokenizer.json (deflated 82%)\n  adding: kaggle/working/mediguide_checkpoint_epoch2/model.safetensors (deflated 7%)\n  adding: kaggle/working/mediguide_checkpoint_epoch2/tokenizer_config.json (deflated 54%)\n  adding: kaggle/working/mediguide_checkpoint_epoch2/merges.txt (deflated 53%)\n  adding: kaggle/working/mediguide_checkpoint_epoch2/config.json (deflated 52%)\n  adding: kaggle/working/mediguide_checkpoint_epoch2/special_tokens_map.json (deflated 60%)\n  adding: kaggle/working/mediguide_checkpoint_epoch2/generation_config.json (deflated 24%)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'mediguide_checkpoint_epoch2.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:58:47.413402Z","iopub.execute_input":"2025-05-25T05:58:47.413928Z","iopub.status.idle":"2025-05-25T05:58:47.418005Z","shell.execute_reply.started":"2025-05-25T05:58:47.413910Z","shell.execute_reply":"2025-05-25T05:58:47.417503Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/mediguide_checkpoint_epoch2.zip","text/html":"<a href='mediguide_checkpoint_epoch2.zip' target='_blank'>mediguide_checkpoint_epoch2.zip</a><br>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"import os\n\npath = \"/kaggle/input/mediguide-checkpoint-chatbot/mediguide_checkpoint_epoch2\"\nprint(os.listdir(path))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T10:55:39.075511Z","iopub.execute_input":"2025-06-06T10:55:39.076122Z","iopub.status.idle":"2025-06-06T10:55:39.088143Z","shell.execute_reply.started":"2025-06-06T10:55:39.076093Z","shell.execute_reply":"2025-06-06T10:55:39.087442Z"}},"outputs":[{"name":"stdout","text":"['config.json', 'merges.txt', 'vocab.json', 'tokenizer_config.json', 'model.safetensors', 'special_tokens_map.json', 'generation_config.json']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Define local model path\nmodel_path = \"/kaggle/input/mediguide-checkpoint-chatbot/mediguide_checkpoint_epoch2\"\n\n# Load model and tokenizer from local files\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    trust_remote_code=True,\n    local_files_only=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_path,\n    trust_remote_code=True,\n    local_files_only=True\n)\n\ntokenizer.pad_token = tokenizer.eos_token\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T05:54:17.851514Z","iopub.execute_input":"2025-05-27T05:54:17.851915Z","iopub.status.idle":"2025-05-27T05:54:19.051131Z","shell.execute_reply.started":"2025-05-27T05:54:17.851890Z","shell.execute_reply":"2025-05-27T05:54:19.050079Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 1024)\n    (wpe): Embedding(1024, 1024)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-23): 24 x GPT2Block(\n        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D(nf=3072, nx=1024)\n          (c_proj): Conv1D(nf=1024, nx=1024)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=4096, nx=1024)\n          (c_proj): Conv1D(nf=1024, nx=4096)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n)"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"import json\nfrom datasets import Dataset\n\n# Load validation data\nwith open(\"/kaggle/input/meddialogue/english-dev.json\", \"r\") as f:\n    val_data = json.load(f)\n\n# Format it into the proper prompt/response format\ndef format_dialogue_data(data):\n    formatted = []\n    for item in data:\n        utterances = item.get(\"utterances\", [])\n        if len(utterances) >= 2 and utterances[0].lower().startswith(\"patient\") and utterances[1].lower().startswith(\"doctor\"):\n            question = utterances[0].replace(\"patient:\", \"\").strip()\n            answer = utterances[1].replace(\"doctor:\", \"\").strip()\n            text = f\"Patient: {question}\\nDoctor:\"\n            formatted.append({\"prompt\": text, \"reference\": answer})\n    return formatted\n\neval_data = format_dialogue_data(val_data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T10:55:49.522929Z","iopub.execute_input":"2025-06-06T10:55:49.523694Z","iopub.status.idle":"2025-06-06T10:55:49.531487Z","shell.execute_reply.started":"2025-06-06T10:55:49.523667Z","shell.execute_reply":"2025-06-06T10:55:49.530611Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"eval_data = eval_data[:100]  # Calculate Rouge score for 100 samples\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T05:54:30.632316Z","iopub.execute_input":"2025-05-27T05:54:30.632673Z","iopub.status.idle":"2025-05-27T05:54:30.637664Z","shell.execute_reply.started":"2025-05-27T05:54:30.632649Z","shell.execute_reply":"2025-05-27T05:54:30.636748Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"model.eval()\n\ngenerated_texts = []\nreference_texts = []\n\nfor sample in eval_data:\n    inputs = tokenizer(sample[\"prompt\"], return_tensors=\"pt\", truncation=True).to(device)\n\n    output_ids = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=50, \n        do_sample=False,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\n    \n    generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    if \"Doctor:\" in generated:\n        generated_answer = generated.split(\"Doctor:\")[-1].strip()\n    else:\n        generated_answer = generated.strip()\n\n    generated_texts.append(generated_answer)\n    reference_texts.append(sample[\"reference\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T10:56:55.894196Z","iopub.execute_input":"2025-06-06T10:56:55.894502Z","iopub.status.idle":"2025-06-06T11:02:11.126377Z","shell.execute_reply.started":"2025-06-06T10:56:55.894480Z","shell.execute_reply":"2025-06-06T11:02:11.125348Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from evaluate import load\n\nrouge = load(\"rouge\")\n\nresults = rouge.compute(predictions=generated_texts, references=reference_texts)\n\n\nprint(\"ROUGE Score is:\")\nfor key, value in results.items():\n    print(f\"{key}: {value:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T11:02:19.305825Z","iopub.execute_input":"2025-06-06T11:02:19.306566Z","iopub.status.idle":"2025-06-06T11:02:21.458488Z","shell.execute_reply.started":"2025-06-06T11:02:19.306533Z","shell.execute_reply":"2025-06-06T11:02:21.457269Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8984dc5deac4163a0520edd7953a095"}},"metadata":{}},{"name":"stdout","text":"ROUGE Score is:\nrouge1: 0.0970\nrouge2: 0.0092\nrougeL: 0.0737\nrougeLsum: 0.0757\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch\nimport math\n\nmodel.eval()\ntotal_loss = 0\nnum_batches = 0\n\nfor sample in eval_data[:100]: \n    inputs = tokenizer(\n        sample[\"prompt\"] + sample[\"reference\"],\n        return_tensors=\"pt\",\n        max_length=512,\n        truncation=True,\n        padding=\"max_length\"\n    ).to(device)\n\n    with torch.no_grad():\n        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n        loss = outputs.loss\n        total_loss += loss.item()\n        num_batches += 1\n\navg_loss = total_loss / num_batches\nperplexity = math.exp(avg_loss)\n\nprint(f\"Avg. Loss: {avg_loss:.4f}\")\nprint(f\"Perplexity : {perplexity:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T11:02:26.119555Z","iopub.execute_input":"2025-06-06T11:02:26.120201Z","iopub.status.idle":"2025-06-06T11:06:02.548943Z","shell.execute_reply.started":"2025-06-06T11:02:26.120176Z","shell.execute_reply":"2025-06-06T11:06:02.548096Z"}},"outputs":[{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"name":"stdout","text":"Avg. Loss: 8.8642\nPerplexity : 7074.03\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from transformers import pipeline\n\n# Create text generation pipeline using your model and tokenizer\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n\n# Sample prompt\nprompt = \"i have cold cough\"\n\n# Generate response\nresponse = pipe(prompt, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n\nprint(\"Model Response:\\n\", response[0][\"generated_text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T11:11:36.193294Z","iopub.execute_input":"2025-06-06T11:11:36.194103Z","iopub.status.idle":"2025-06-06T11:11:45.847373Z","shell.execute_reply.started":"2025-06-06T11:11:36.194074Z","shell.execute_reply":"2025-06-06T11:11:45.846564Z"}},"outputs":[{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"Model Response:\n i have cold coughs, have vomiting and diarrhea, or are pregnant. Although this condition is not serious, anemia may prevent an entire pregnancy from being successfully delivered.\n\nTardive dyskinesia is a disorder associated with excessive breathing followed by loss of awareness, resulting in a severe, chronic inability to speak. The syndrome's onset generally occurs during childhood or adolescence. The patient has the most difficulty with swallowing, with vomiting being much more frequent. A complete absence of speech may continue for months, often\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}